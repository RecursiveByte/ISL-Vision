# ðŸ¤Ÿ ISL-Vision

Real-time Indian Sign Language to text converter using computer vision and deep learning.

![React](https://img.shields.io/badge/React-19.2.0-61DAFB?logo=react)
![Python](https://img.shields.io/badge/Python-3.10-3776AB?logo=python)
![TailwindCSS](https://img.shields.io/badge/Tailwind-3.4.17-38B2AC?logo=tailwind-css)

---
## Demo
**ISL-Vision** [Live demo](https://isl-vision.onrender.com)


## What It Does

Captures hand gestures via webcam, detects 21 hand landmarks using MediaPipe, and predicts ISL letters using a trained DNN model. Auto-builds words from detected letters with manual editing controls.

---

## Tech Stack

**Frontend:**
- React 19.2 + Vite 5.4
- Tailwind CSS 3.4
- MediaPipe Hands (CDN)
- Axios

**Backend:**
- Fast api
- TensorFlow/Keras (DNN model)
- OpenCV + MediaPipe
- Pandas,NumPy,scikit-learn ...

**Model:**
- Trained in Google Colab using TensorFlow
- Deep Neural Network architecture
- Hand landmark feature extraction
- StandardScaler normalization

---

## Project Structure

```
AI_ML/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                          # FAST API server
â”‚   â”œâ”€â”€ main.py                         # Alternative entry
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ runtime.txt
â”‚   â”œâ”€â”€ basic/
â”‚   â”‚   â”œâ”€â”€ gesture.py                  # Gesture recognition logic
â”‚   â”‚   â””â”€â”€ sign.py                     # Sign language processing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ hand_landmark_dnn.keras     # Trained DNN model
â”‚   â”‚   â””â”€â”€ scaler.pkl                  # Feature scaler
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ index.html                  # Demo page
â”‚   â”‚   â”œâ”€â”€ script.js
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py                   # Configuration
â”‚       â”œâ”€â”€ feature_extractor.py        # Extract hand features
â”‚       â”œâ”€â”€ hand_detector.py            # MediaPipe detector
â”‚       â”œâ”€â”€ model_manager.py            # Model loading
â”‚       â”œâ”€â”€ visualization.py            # Drawing utilities
â”‚       â””â”€â”€ word_builder.py             # Word building logic
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ package.json
    â”œâ”€â”€ vite.config.js
    â”œâ”€â”€ tailwind.config.js
    â”œâ”€â”€ postcss.config.js
    â”œâ”€â”€ index.html
    â”œâ”€â”€ public/
    â”‚   â””â”€â”€ vite.svg
    â””â”€â”€ src/
        â”œâ”€â”€ main.jsx                    # Entry point
        â”œâ”€â”€ App.jsx                     # Main component
        â”œâ”€â”€ index.css                   # Global styles
        â”œâ”€â”€ components/
        â”‚   â”œâ”€â”€ VideoCanvas.jsx         # Webcam + canvas overlay
        â”‚   â”œâ”€â”€ StatusPanel.jsx         # Connection/prediction status
        â”‚   â”œâ”€â”€ WordDisplay.jsx         # Current word display
        â”‚   â”œâ”€â”€ WordControls.jsx        # Space/delete/clear buttons
        â”‚   â”œâ”€â”€ WebcamControls.jsx      # Start/stop webcam
        â”‚   â”œâ”€â”€ ColorLegend.jsx         # Landmark color guide
        â”‚   â””â”€â”€ index.js                # Component exports
        â”œâ”€â”€ hooks/
        â”‚   â”œâ”€â”€ useWebcam.js            # Webcam + API logic
        â”‚   â”œâ”€â”€ useMediaPipe.js         # Hand detection
        â”‚   â”œâ”€â”€ useWordBuilder.js       # Word building state
        â”‚   â””â”€â”€ useBackendConnection.js # Health check
        â”œâ”€â”€ utils/
        â”‚   â””â”€â”€ handDrawing.js          # Canvas drawing (21 landmarks)
        â””â”€â”€ config/
            â””â”€â”€ constants.js            # API URL, intervals, MediaPipe config
```


## How It Works

1. **Webcam** â†’ Captures video (1280x720)
2. **MediaPipe** â†’ Detects 21 hand landmarks per hand
3. **Frontend** â†’ Draws landmarks on canvas, sends frame to backend every 1s
4. **Backend** â†’ Extracts features, runs through DNN model
5. **Prediction** â†’ Returns ISL letter + confidence score
6. **Word Building** â†’ Auto-adds letter every 5s, manual controls available

**Configuration** (`constants.js`):
```javascript
PREDICTION_INTERVAL = 1000   // Backend call frequency (ms)
LETTER_ADD_INTERVAL = 5000   // Auto-add frequency (ms)
MEDIAPIPE_CONFIG = {
  maxNumHands: 2,
  modelComplexity: 1,
  minDetectionConfidence: 0.3,
  minTrackingConfidence: 0.3
}
```

---

## API Endpoints

- `GET /health` - Backend health check
- `POST /api/predict` - Send frame (base64 JPEG), get prediction

**Response:**
```json
{
  "predicted_letter": "A",
  "confidence": 0.94,
  "hand_count": 1
}
```

---

## Model Details

- **Architecture:** Deep Neural Network (DNN)
- **Training:** Google Colab with TensorFlow/Keras
- **Input:** 21 hand landmarks (x, y, z) Ã— 2 hands = 126 features
- **Output:** ISL letter classification
- **Preprocessing:** StandardScaler normalization
- **Format:** Keras (.keras) + pickle (.pkl)

---

## Key Features

- Real-time hand tracking (30+ FPS)
- Dual hand support
- Color-coded landmark visualization
- Auto word building with countdown
- Manual editing (space, delete, clear)
- Confidence scoring
- Responsive UI

---

## License

MIT License

---

## Author

**RecursiveByte** - [GitHub](https://github.com/RecursiveByte)
